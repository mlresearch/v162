---
title: 'Model soups: averaging weights of multiple fine-tuned models improves accuracy
  without increasing inference time'
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: The conventional recipe for maximizing model accuracy is to (1) train multiple
  models with various hyperparameters and (2) pick the individual model which performs
  best on a held-out validation set, discarding the remainder. In this paper, we revisit
  the second step of this procedure in the context of fine-tuning large pre-trained
  models, where fine-tuned models often appear to lie in a single low error basin.
  We show that averaging the weights of multiple models fine-tuned with different
  hyperparameter configurations often improves accuracy and robustness. Unlike a conventional
  ensemble, we may average many models without incurring any additional inference
  or memory costs—we call the results “model soups.” When fine-tuning large pre-trained
  models such as CLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides
  significant improvements over the best model in a hyperparameter sweep on ImageNet.
  The resulting ViT-G model, which attains 90.94% top-1 accuracy on ImageNet, achieved
  a new state of the art. Furthermore, we show that the model soup approach extends
  to multiple image classification and natural language processing tasks, improves
  out-of-distribution performance, and improves zero-shot performance on new downstream
  tasks. Finally, we analytically relate the performance similarity of weight-averaging
  and logit-ensembling to flatness of the loss and confidence of the predictions,
  and validate this relation empirically. Code is available at https://github.com/mlfoundations/model-soups.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wortsman22a
month: 0
tex_title: 'Model soups: averaging weights of multiple fine-tuned models improves
  accuracy without increasing inference time'
firstpage: 23965
lastpage: 23998
page: 23965-23998
order: 23965
cycles: false
bibtex_author: Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Ya and Roelofs,
  Rebecca and Gontijo-Lopes, Raphael and Morcos, Ari S and Namkoong, Hongseok and
  Farhadi, Ali and Carmon, Yair and Kornblith, Simon and Schmidt, Ludwig
author:
- given: Mitchell
  family: Wortsman
- given: Gabriel
  family: Ilharco
- given: Samir Ya
  family: Gadre
- given: Rebecca
  family: Roelofs
- given: Raphael
  family: Gontijo-Lopes
- given: Ari S
  family: Morcos
- given: Hongseok
  family: Namkoong
- given: Ali
  family: Farhadi
- given: Yair
  family: Carmon
- given: Simon
  family: Kornblith
- given: Ludwig
  family: Schmidt
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/wortsman22a/wortsman22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
