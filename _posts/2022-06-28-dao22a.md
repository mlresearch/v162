---
title: 'Monarch: Expressive Structured Matrices for Efficient and Accurate Training'
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: 'Large neural networks excel in many domains, but they are expensive to
  train and fine-tune. A popular approach to reduce their compute or memory requirements
  is to replace dense weight matrices with structured ones (e.g., sparse, low-rank,
  Fourier transform). These methods have not seen widespread adoption (1) in end-to-end
  training due to unfavorable efficiencyâ€“quality tradeoffs, and (2) in dense-to-sparse
  fine-tuning due to lack of tractable algorithms to approximate a given dense weight
  matrix. To address these issues, we propose a class of matrices (Monarch) that is
  hardware-efficient (they are parameterized as products of two block-diagonal matrices
  for better hardware utilization) and expressive (they can represent many commonly
  used transforms). Surprisingly, the problem of approximating a dense weight matrix
  with a Monarch matrix, though nonconvex, has an analytical optimal solution. These
  properties of Monarch matrices unlock new ways to train and fine-tune sparse and
  dense models. We empirically validate that Monarch can achieve favorable accuracy-efficiency
  tradeoffs in several end-to-end sparse training applications: speeding up ViT and
  GPT-2 training on ImageNet classification and Wikitext-103 language modeling by
  2x with comparable model quality, and reducing the error on PDE solving and MRI
  reconstruction tasks by 40%. In sparse-to-dense training, with a simple technique
  called "reverse sparsification," Monarch matrices serve as a useful intermediate
  representation to speed up GPT-2 pretraining on OpenWebText by 2x without quality
  drop. The same technique brings 23% faster BERT pretraining than even the very optimized
  implementation from Nvidia that set the MLPerf 1.1 record. In dense-to-sparse fine-tuning,
  as a proof-of-concept, our Monarch approximation algorithm speeds up BERT fine-tuning
  on GLUE by 1.7x with comparable accuracy.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: dao22a
month: 0
tex_title: 'Monarch: Expressive Structured Matrices for Efficient and Accurate Training'
firstpage: 4690
lastpage: 4721
page: 4690-4721
order: 4690
cycles: false
bibtex_author: Dao, Tri and Chen, Beidi and Sohoni, Nimit S and Desai, Arjun and Poli,
  Michael and Grogan, Jessica and Liu, Alexander and Rao, Aniruddh and Rudra, Atri
  and Re, Christopher
author:
- given: Tri
  family: Dao
- given: Beidi
  family: Chen
- given: Nimit S
  family: Sohoni
- given: Arjun
  family: Desai
- given: Michael
  family: Poli
- given: Jessica
  family: Grogan
- given: Alexander
  family: Liu
- given: Aniruddh
  family: Rao
- given: Atri
  family: Rudra
- given: Christopher
  family: Re
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/dao22a/dao22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
